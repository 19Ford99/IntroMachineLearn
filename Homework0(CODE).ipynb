{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf0e643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c40df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = pd.read_csv(\"D3.csv\") # Reads csv file and sets it to the variable D3\n",
    "\n",
    "X1 = np.array(DF.values[:,0]) # 1st input column\n",
    "X2 = np.array(DF.values[:,1]) # 2nd input column\n",
    "X3 = np.array(DF.values[:,2]) # 3rd input column\n",
    "\n",
    "y  = np.array(DF.values[:,3]) # Last column, output\n",
    "m = len(y) # Number of values in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f13db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X,y,theta):\n",
    "#    **** Computes the loss function for linear regression ****\n",
    "    h = X.dot(theta) # h = predictions\n",
    "    errors = np.subtract(h,y)\n",
    "    sqrErrors = np.square(errors)\n",
    "    J = 1/(2*m) * np.sum(sqrErrors)\n",
    "    return J\n",
    "\n",
    "def gradient_descent(X,y,theta,alpha,iterations):\n",
    "    loss_history = np.zeros(iterations)\n",
    "    for i in range(iterations):\n",
    "        h = X.dot(theta)\n",
    "        errors = np.subtract(h,y)\n",
    "        sum_delta = (alpha/m)*X.transpose().dot(errors);\n",
    "        theta = theta - sum_delta;\n",
    "        loss_history[i] = compute_loss(X,y,theta)\n",
    "    return theta, loss_history\n",
    "    \n",
    "def predict(X, theta):\n",
    "    return theta[0] + theta[1]*X[0] + theta[2]*X[1] + theta[3]*X[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835efdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1\n",
    "# Inputs Vs Outputs\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "plt.figure()\n",
    "plt.scatter(X1, y, color ='red', marker= '+', label = 'Training Data')\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Figure 1: y Vs X1 \")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X2, y, color ='green', marker= '+', label = 'Training Data')\n",
    "plt.xlabel(\"X2\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Figure 2: y Vs X2\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X3, y, color ='orange', marker= '+', label = 'Training Data')\n",
    "plt.xlabel(\"X3\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Figure 3: y Vs X3\")\n",
    "plt.grid()\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e82e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix with a single column of ones\n",
    "X0 = np.ones((m,1))\n",
    "\n",
    "# Using reshape function to convert X1,X2,X3 into a 2D Array\n",
    "X1 = X1.reshape(m, 1)\n",
    "X2 = X2.reshape(m, 1)\n",
    "X3 = X3.reshape(m, 1)\n",
    "\n",
    "# Using hstack() function to stack X_0,X_1,X_3 horizontally\n",
    "X1 = np.hstack((X0, X1)) \n",
    "X2 = np.hstack((X0, X2)) \n",
    "X3 = np.hstack((X0, X3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896066a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(2)\n",
    "\n",
    "# Compute the cost for theta values \n",
    "loss1 = compute_loss(X1, y, theta)\n",
    "print('The cost for X1 =', loss1)\n",
    "loss2 = compute_loss(X2, y, theta)\n",
    "print('The cost for X2 =', loss2)\n",
    "loss3 = compute_loss(X3, y, theta) \n",
    "print('The cost for X3 =', loss3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bb5322",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = [0. , 0.]\n",
    "iterations = 300; \n",
    "alpha = 0.05;\n",
    "theta1, loss1_history = gradient_descent(X1, y, theta, alpha, iterations) \n",
    "print('Final value of theta1 =', theta1) \n",
    "\n",
    "iterations = 300; \n",
    "alpha = 0.05;\n",
    "theta2, loss2_history = gradient_descent(X2, y, theta, alpha, iterations) \n",
    "print('Final value of theta2 =', theta2)\n",
    "\n",
    "iterations = 300; \n",
    "alpha = 0.05;\n",
    "theta3, loss3_history = gradient_descent(X3, y, theta, alpha, iterations) \n",
    "print('Final value of theta3 =', theta3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a4fc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the model for X1\n",
    "plt.figure()\n",
    "plt.scatter(X1[:,1], y, color='red', marker= '+', label= 'Training Data') \n",
    "plt.plot(X1[:,1],X1.dot(theta1), color='blue', label='Linear Regression') \n",
    "plt.grid() \n",
    "plt.xlabel('X1') \n",
    "plt.ylabel('h (x)') \n",
    "plt.title('Linear Regression Fit for X1') \n",
    "plt.legend()\n",
    "\n",
    "# Plots the model for X2\n",
    "plt.figure()\n",
    "plt.scatter(X2[:,1], y, color='green', marker= '+', label= 'Training Data') \n",
    "plt.plot(X2[:,1],X2.dot(theta2), color='blue', label='Linear Regression') \n",
    "plt.grid() \n",
    "plt.xlabel('X2') \n",
    "plt.ylabel('h (x)') \n",
    "plt.title('Linear Regression Fit for X2') \n",
    "plt.legend()\n",
    "\n",
    "# Plots the model for X3\n",
    "plt.figure()\n",
    "plt.scatter(X3[:,1], y, color='orange', marker= '+', label= 'Training Data') \n",
    "plt.plot(X3[:,1],X3.dot(theta3), color='blue', label='Linear Regression') \n",
    "plt.grid() \n",
    "plt.xlabel('X3') \n",
    "plt.ylabel('h (x)') \n",
    "plt.title('Linear Regression Fit for X3') \n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28f8a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the loss history for X1\n",
    "plt.figure()\n",
    "plt.plot(loss1_history[0:len(loss1_history)], color='blue') \n",
    "plt.grid() \n",
    "plt.xlabel('Number of iterations') \n",
    "plt.ylabel('Cost (J)') \n",
    "plt.title('Convergence of gradient descent for X1')\n",
    "\n",
    "# Plots the loss history for X2\n",
    "plt.figure()\n",
    "plt.plot(loss2_history[0:len(loss2_history)], color='blue') \n",
    "plt.grid() \n",
    "plt.xlabel('Number of iterations') \n",
    "plt.ylabel('Cost (J)') \n",
    "plt.title('Convergence of gradient descent for X2')\n",
    "\n",
    "# Plots the loss history for X3\n",
    "plt.figure()\n",
    "plt.plot(loss3_history[0:len(loss3_history)], color='blue') \n",
    "plt.grid() \n",
    "plt.xlabel('Number of iterations') \n",
    "plt.ylabel('Cost (J)') \n",
    "plt.title('Convergence of gradient descent X3');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc85b1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 2\n",
    "D3 = pd.read_csv(\"D3.csv\") # Reads csv file and sets it to the variable D3\n",
    "\n",
    "X1 = np.array(D3.values[:,0]) # 1st input column\n",
    "X2 = np.array(D3.values[:,1]) # 2nd input column\n",
    "X3 = np.array(D3.values[:,2]) # 3rd input column\n",
    "\n",
    "y  = np.array(D3.values[:,3]) # Last column, output\n",
    "m = len(y) # Number of values in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5474b444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix with a single column of ones\n",
    "X0 = np.ones((m,1))\n",
    "\n",
    "# Using reshape function to convert X1,X2,X3 into a 2D Array\n",
    "X1 = X1.reshape(m, 1)\n",
    "X2 = X2.reshape(m, 1)\n",
    "X3 = X3.reshape(m, 1)\n",
    "\n",
    "# Using hstack() function to stack X_0,X_1,X_3 horizontally\n",
    "X = np.hstack((X0, X1, X2, X3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a527f846",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(4)\n",
    "\n",
    "# Compute the cost for theta values \n",
    "loss = compute_loss(X, y, theta)\n",
    "print('The cost for X =', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09018675",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iterations = 300; \n",
    "alpha = 0.05;\n",
    "theta, loss_history = gradient_descent(X, y, theta, alpha, iterations) \n",
    "print('Final value of theta =', theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac306be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the loss history for X\n",
    "plt.figure()\n",
    "plt.plot(range(1, iterations + 1),loss_history, color='blue') \n",
    "plt.grid() \n",
    "plt.xlabel('Number of iterations') \n",
    "plt.ylabel('Cost (J)') \n",
    "plt.title('Convergence of gradient descent for X');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3780bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction for new values\n",
    "\n",
    "# new_X = (1,1,1)\n",
    "new_X = np.array([1,1,1])\n",
    "y_pred = predict(new_X, theta)\n",
    "print(\"For values (1,1,1): \", y_pred)\n",
    "\n",
    "# new_X = (2,0,4)\n",
    "new_X = np.array([2,0,4])\n",
    "y_pred = predict(new_X, theta)\n",
    "\n",
    "print(\"For values (2,0,4): \", y_pred)\n",
    "\n",
    "# new_X = (3,2,1)\n",
    "new_X = np.array([3,2,1])\n",
    "y_pred = predict(new_X, theta)\n",
    "\n",
    "print(\"For values (3,2,1): \",y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
